% $ platex body
% $ dvipdfmx body

\documentclass[a4j]{jarticle}

\title{「データ解析入門」レポート}
\author{1677281 山内 拓弥}
\date{平成29年1月19日}
\usepackage{colortbl}
\usepackage[dvipdfmx]{graphicx}
\renewcommand{\thesection}{課題\arabic{section}}
\renewcommand{\thesubsection}{}

\begin{document}

\maketitle

\section{}
クラスタリングとベクトル量子化の定義を、
その違いがわかるように説明し、
平方和最小基準クラスタリング問題と2乗誤差に基づくベクトル量子化問題の
局所最適解における性質について記述しなさい。

\subsection{【解答】}
クラスタリングとは、類似したデータをクラスタ(Cluster:群れ、集団)
としてまとめることである。
解は、データの分類として表現される。
ベクトル量子化とは、入力ベクトル集合を代表するプロトタイプベクトル、
言い換えれば代表ベクトルを求めることである。
解は、ベクトルとして表現される。
平方和と2乗誤差を考えた時、クラスタリング問題の局所最適解${\cal C}$は、
解${\cal C}$で決まる重心ベクトル$\mbox{\boldmath $\mu$}_k$
の集合を代表ベクトルとするベクトル量子化問題の解${\cal W}$
のボロノイ分割で決まる分割${\cal C'}$と一致する。
この意味において、局所最適解における両問題の解は実質的に等価である。

\section{}
教科書の「HokkaidoTowns\_xy\@.dat」データを、 k-meansアルゴリズムと競合学習で、
6分割にクラスタリングする実験を行い、 その結果を示し、考察しなさい。
具体的には、プログラムに与える引数である「乱数の種」、
「学習率」(競合学習のみ)などをいくつか変更して実験を行い、
両手法の性能比較を行いなさい。

\subsection{【解答】}

\paragraph{[乱数の種を振り、k-meansアルゴリズムと競合学習を比較]}
本実験では、2乗誤差、もしくは、平方和である$E_Q$と$J_W$の傾向を、
k-meansアルゴリズムと競合学習とで比較した。
k-meansアルゴリズム、競合学習ともに、乱数を変えながら10回試行した。
競合学習の学習率は0.001とした。
結果を表\ref{tab:k-mean-comp}に示す。
競合学習の結果は安定している一方、
k-meansアルゴリズムは、表\ref{tab:k-mean-comp}のグレーのセルに示すように、
悪い結果が出ることがある。

\begin{table}[tbp]
 \begin{center}
  \caption{k-meansアルゴリズム、競合学習、2乗誤差比較}
  \label{tab:k-mean-comp}
  \begin{tabular}{|r||r|r|r|} \hline
            & k-means     & \multicolumn{2}{|c|}{競合学習} \\ \cline{2-4}
   試行番号 & $E_Q = J_W$                  & $E_Q$  & $J_W$  \\ \hline \hline
          1 &                       474825 & 472490 & 472288 \\ \hline
          2 & \cellcolor[gray]{0.8} 543956 & 472673 & 472217 \\ \hline
          3 &                       474825 & 472577 & 472288 \\ \hline
          4 & \cellcolor[gray]{0.8} 609216 & 472959 & 472808 \\ \hline
          5 &                       473367 & 472557 & 472288 \\ \hline
          6 &                       473367 & 472501 & 472288 \\ \hline
          7 &                       472288 & 472439 & 472217 \\ \hline
          8 & \cellcolor[gray]{0.8} 503663 & 472648 & 472288 \\ \hline
          9 &                       472769 & 472429 & 472288 \\ \hline
         10 &                       474818 & 472367 & 472217 \\ \hline
  \end{tabular}
 \end{center}
\end{table}

k-meansアルゴリズムの悪い結果は、
局所最適解になっていると考えられる。
良い結果の一例として試行番号1
のクラスタリング結果を図\ref{fig:Hokkaido_xyl_seed1}に示す。
また、悪い結果の一例として試行番号4
のクラスタリング結果を図\ref{fig:Hokkaido_xyl_seed4}に示す。
これらの間には、
\begin{itemize}
\item 図\ref{fig:Hokkaido_xyl_seed4}ではCluster3と4が広い。
\item 図\ref{fig:Hokkaido_xyl_seed1}のCluster5に相当する地域は、
      図\ref{fig:Hokkaido_xyl_seed4}ではCluster2と5に分割されている。
\end{itemize}
といった差異が見られる。
また、競合学習のクラスタリング結果を図\ref{fig:Hokkaido_xyl_compLearn}
に示す。k-meansアルゴリズムの良い結果である図\ref{fig:Hokkaido_xyl_seed1}
と似ていることがわかる。

\begin{figure}[tbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/Hokkaido_xyl_seed1.eps}
  \end{center}
  \caption{k-meanアルゴリズム、良い結果}
  \label{fig:Hokkaido_xyl_seed1}
 \end{minipage}
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/Hokkaido_xyl_seed4.eps}
  \end{center}
  \caption{k-meanアルゴリズム、悪い結果}
  \label{fig:Hokkaido_xyl_seed4}
 \end{minipage}
\end{figure}

\paragraph{[競合学習の学習率と2乗誤差の関係]}

学習率を振って、反復終了時の2乗誤差$E_Q$と$J_W$を調べた。
結果を図\ref{fig:compLearn}に示す。
横軸は学習率、縦軸は$E_Q$と$J_W$である。
学習率が$10^{-4}$から$10^{-2}$の範囲では、$E_Q$、$J_W$
共に$4.7\times 10^5$付近で安定している。
これより学習率が小さくても大きくても、反復終了時の$E_Q$、$J_W$は大きく、
すなわち悪くなる。
学習率が小さい時は、重みベクトルが収束し切らないため、
反復終了時の$E_Q$、$J_W$が大きくなっていると考えられる。
学習率が大きい時は、局所最適解の近傍まではたどり着くが、
1反復あたりの重みベクトルの変化量が大きく、
局所最適解に近づき切れないため$E_Q$、$J_W$
が大きい値にとどまっていると考えられる。

\begin{figure}[tbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/Hokkaido_xyl_compLearn.eps}
  \end{center}
  \caption{競合学習、学習率$=0.001$のクラスタリング結果}
  \label{fig:Hokkaido_xyl_compLearn}
 \end{minipage}
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/compLearn.eps}
  \end{center}
  \caption{競合学習、学習率と$E_Q$、$J_W$}
  \label{fig:compLearn}
 \end{minipage}
\end{figure}

\section{}
教科書の「HokkaidoTowns\_xy\@.dat」データを、
k-meansアルゴリズムなどで6分割したクラスタリングを行い、
その結果を「Hokkaido\_xyl\@.dat」としなさい。
次に、修正パーセプトロン学習プログラムであるmodPerceptronを用い、
上記クラスタリング結果を学習パターンとして学習させなさい。
その時、「乱数の種」、「学習率」を変化させて実験し、
収束するまでの繰り返し回数の傾向について考察しなさい。

\subsection{【解答】}
図\ref{fig:modPerceptron}に収束回数を示す。
横軸は学習率、縦軸は反復回数である。
反復回数は、乱数の種を変えながら10回試行し、その平均値をプロットしている。
学習率が0.1から0.3の範囲では、反復回数は50未満に収まっている。
学習率がこの範囲より大きくても小さくても、反復回数は大きい、
すなわち収束しづらくなっていることがわかる。
学習率が小さい場合、重みベクトル更新時の変化量が少なく、
収束に時間がかかるといった原因が考えられる。
学習率が大きい場合、重みベクトル更新時に変化が大きすぎて、
最適点を飛び越えてしまい、収束しづらくなっていると考えられる。

\begin{figure}[tbp]
 \begin{center}
  \includegraphics[width=0.5\hsize]{fig/modPerceptron.eps}
 \end{center}
 \caption{modPerceptron、収束までの反復回数}
 \label{fig:modPerceptron}
\end{figure}


\section{}
教科書の「HokkaidoTowns\_xy\@.dat」の各カラムがx y座標の座標値を表すとします。
このデータの重みベクトルによるボロノイ分割が、
y軸を境界とする2分割になりました。
この重みベクトルはどのようなものになるか例を使って説明しなさい。

\subsection{【解答】}
図\ref{fig:problem04}に例を示す。黒塗りの点が重みベクトルを示す。
白抜きの点は、各市町村である。中央の実線が$y$軸であり、
2つの重みベクトルの垂直二等分線になる。
すなわち、重みベクトルは、$y$軸に関して線対称になる。

\begin{figure}[tbp]
 \begin{center}
  \includegraphics[width=0.5\hsize]{fig/problem04.eps}
 \end{center}
 \caption{課題4の例}
 \label{fig:problem04}
\end{figure}

\section{}
赤と青の2つの箱があり、
赤の箱にはりんごが2個とオレンジが6個、
青の箱にはりんごが3個とオレンジが1個入っているとする。
箱の1つをランダムに選び、果物をランダムに1個取り出す。
その際、赤の箱を40\%、青の箱を60\%選び、
箱の中の果物は別け隔てなく同じ確からしさで選ぶ。
この時、オレンジを選び出したとして、
それが青い箱から取り出されたものである確率はいくらか?
導出過程も含めて書きなさい。

\subsection{【解答】}
青い箱を選ぶ事象を$B$、オレンジを選ぶ事象を$O$とすると、
求める量は$P(B|O)$である。ベイズの定理より、
\begin{equation}
\label{p04sol}
P(B|O)=\frac{P(B)P(O|B)}{P(O)}
\end{equation}
題意より
\begin{equation}
P(B) = \frac{3}{5}, P(B^C)=\frac{2}{5}, P(O|B)=\frac{1}{4}, P(O|B^C)=\frac{3}{4}
\end{equation}
である。右肩の$C$は余事象を表す。例えば、$B^C$は赤い箱を選ぶ事象である。
これらを式(\ref{p04sol})に代入して、
\begin{equation}
(\textrm{式(\ref{p04sol})の分子})=\frac{3}{5}\times\frac{1}{4}=\frac{3}{20}
\end{equation}
\begin{eqnarray}
(\textrm{式(\ref{p04sol})の分母})&=&P(O) \\
&=& P(O,B)+P(O,B^C) \\
&=& P(B)P(O|B)+P(B^C)P(O|B^C) \\
&=& \frac{3}{5}\times\frac{1}{4}+\frac{2}{5}\times\frac{3}{4} \\
&=& \frac{3}{20}+\frac{6}{20} \\
&=& \frac{9}{20}
\end{eqnarray}
したがって、答えは
\begin{equation}
\frac{3}{20}\times\frac{20}{9} = \frac{1}{3}
\end{equation}
となる。

\section{}
教科書の表5\@.2「フルーツのブレンド割合」の値は、
壷を選んだときに各フルーツが取り出される条件付確率を表しています。
今、各壷が選ばれる確率がすべて等しいとします。
この時、下記のフルーツポンチがあるとき、
それぞれの壷が選ばれたという「対数事後確率の大小関係を比較できる値」
を求め比較しなさい。

\subsection{【解答】}
\begin{equation}
\log P(C^k) + \sum_{m}x_m\log q_m^k
\end{equation}
を計算すればよい。ここで、$C^k$は壷$k$を選ぶ事象、$P(C^k)$はその確率、
$m$は果物の番号(たとえばマンゴーなら$m=1$、ナタデココなら$m=2$など)、
$x_m$は果物$m$の個数、$q_m^k$は壷$k$の中の果物$m$の割合である。
全ての$k$において$P(C^k)$は一定なので、これは無視してよく、結局
\begin{equation}
\sum_{m}x_m\log q_m^k
\end{equation}
を計算し、比較すればよい。\\
トロピカルの場合、
\begin{equation}
\log 0.5 + 2\log 0.1 + 3\log 0.03 + 5\log 0.04 \simeq -31.9
\end{equation}
クラシックの場合、
\begin{equation}
\log 0.05 + 2\log 0.1 + 3\log 0.1 + 5\log 0.1 \simeq -26.0
\end{equation}
山形の場合、
\begin{equation}
\log 0.03 + 2\log 0.1 + 3\log 0.15 + 5\log 0.4 \simeq -18.4
\end{equation}
山梨の場合、
\begin{equation}
\log 0.05 + 2\log 0.1 + 3\log 0.5 + 5\log 0.05 \simeq -24.7
\end{equation}
となる。従って、山形の壷である可能性が最も高い。

\section{}
フルーツポンチをgenPunchで生成し、
それをestParamで処理して、
モデルのパラメータを推定する実験をしなさい。
ボウルの数や果物の数を変えると、
推定結果の精度にどのような影響があるかについて考察しなさい。

\subsection{【解答】}

\paragraph{[ボウル当りの果物数を固定、ボウル数を振る実験]}
ボウル当りの果物数を200に固定し、
ボウル数を振って、パラメータ推定値の誤差を調べた。
結果を図\ref{fig:punch_bowl}に示す。
横軸はボウル数、縦軸は下に示す数式で示される規格化誤差2乗和である。 \\
図\ref{fig:punch_bowl}で「fruit」と示されているプロットは、
果物の割合の推定値の規格化誤差2乗和であり、
\begin{equation}
\label{err_fruit}
\sum_{k} \sum_{m} (\frac{p_m^k}{q_m^k} - 1)^2
\end{equation}
と表される量である。
$q_m^k$ は壷$k$に含まれる果物$m$の割合のパラメータである。
なお、$q_m^k$は本来推定対象で、通常は未知数であるが、
今回はアルゴリズムの性能評価のために既知であるこの値を積極的に利用している。
$p_m^k$ はパラメータの推定値である。
すなわち、推定値を既知のパラメータで規格化したあと、
誤差の2乗和をとって、それを指標としている。
同様に、図\ref{fig:punch_bowl}で「pot」と示されているプロットは、
壷を選ぶ確率の推定値の規格化誤差2乗和であり、
\begin{equation}
\label{err_pot}
\sum_{k} (\frac{c^k}{C^k} - 1)^2
\end{equation}
と表される量である。$c^k$は、壷$k$が選ばれる確率の推定値、
$C^k$はそのパラメータである。
果物の割合の推定値(図\ref{fig:punch_bowl}上のfruit)、
壷の選ばれる確率の推定値(図\ref{fig:punch_bowl}上のpot)
共に、ボウル数の増加に従い誤差が小さくなる傾向が見られる。
データ数が増えることにより、推定精度が高まっているためであると考えられる。

\begin{figure}[tbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/punch_pot.eps}
  \end{center}
  \caption{ボウル数を振った時の誤差の変化}
  \label{fig:punch_bowl}
 \end{minipage}
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=\hsize]{fig/punch_fruit.eps}
  \end{center}
  \caption{果物数を振った時の誤差の変化}
  \label{fig:punch_fruit}
 \end{minipage}
\end{figure}

\paragraph{[ボウル数を固定し、ボウル当りの果物数を振る実験]}
ボウル数を200に固定し、
ボウル当りの果物数を振って、パラメータ推定値の誤差を調べた。
結果を図\ref{fig:punch_fruit}に示す。
横軸はボウル当りの果物数、
縦軸は、先述の実験と同様、式(\ref{err_fruit})および(\ref{err_pot})
で表される規格化誤差2乗和である。
果物の割合の推定値(図\ref{fig:punch_fruit}上のfruit)は、
ボウル当りの果物数の増加に従い誤差が小さくなる傾向が見られる。
壷の選択される確率の推定値(図\ref{fig:punch_fruit}上のpot)は、
ボウル当りの果物数を増加させると推定精度にある程度の向上傾向は見られるが、
図\ref{fig:punch_bowl}と同程度には向上しない。
ボウル数を固定しているため、
壷の選択確率推定精度にあまり寄与しないためであると考えられる。

\end{document}
